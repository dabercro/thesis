\chapter{Corrections and Uncertainties}

Despite efforts to simulate LHC collisions accurately,
a number of differences in the distributions predicted by MC and present in data arise.
There are many reasons for this.
The theory calculations used for simulation are not accurate since only leading order or
next-to-leading order perturbations are used by the generators.
After the generator step, the detector must be simulated,
which is difficult due to the many composite parts,
both active detector material and inactive electronics, that must be modeled in \texttt{GEANT4}.
This is made more difficult since the detector degrades in its high radiation environment.
The relation of generating particles and detector geometry also depends on the beam conditions,
which are also not predicted exactly.
The most accurate measurements of the beam profile are destructive to the beam,
so measurements cannot be made during the run.
Predictions from simulations are also made harder because there are so many events recorded at the LHC,
so tails will be generated in obscure areas of phase space.
These tails are difficult to model correctly, yet often contain the most interesting events.
Corrections are made to the simulation by re-weighting based on detector conditions and
efficiencies of particle identification,
and by scaling the predicted energies based on particle type.

\section{Efficiency Scale Factors}

The identification of particles based on working points happen at different rates
between simulation and collected data.
These efficiencies are measured separately in data and Monte Carlo.
The difference depends on a scale factor,
which is measured as a function of particle energy and location within the detector.
Inclusive selections are used for the measurement,
which is then transferred to the events in the analysis.
Since this analysis relies on counting leptons for its categorization,
selection efficiency for both muons and electrons must be measured.
A scale factor for the MET trigger efficiency is also derived for the 0-lepton categories.

\subsection{Muons}

To remove fake muons from events, muons are selected in three ways.
Muon identification cuts are applied, isolation cuts are applied,
and certain triggers are required.
These three things each behave differently in MC and data.
For each of these, a separate efficiency is derived in MC and data,
and then a total scale factor is derived via the following formula.
\begin{gather}
  \epsilon^\mu = \epsilon^\mu_\mathrm{ID} \times \epsilon^\mu_\mathrm{ISO|ID} \times \epsilon^\mu_\mathrm{Trig|ISO}
\end{gather}
The scale factor for muon ID, which is tight for positively counting muons,
and loose for counting muons to veto, is $\epsilon^\mu_\mathrm{ID}$.
Passing the isolation cut, given the ID is scaled by $\epsilon^\mu_\mathrm{ISO|ID}$,
and the scale factor for the trigger, after passing the isolation cut is
$\epsilon^\mu_\mathrm{Trig|ISO}$.

These are each separately measured via the tag and probe method.
A single tag muon is selected in events with a single muon trigger.
For events with a second, oppositely-charged muon
that reconstructs the $Z$ boson resonance with the first,
the second muon, called a probe, is checked for identification efficiency.
Since the probe muon does not rely on a categorization for the selection,
only on the successful reconstruction of a $Z$ boson,
the collection of probe muons make up an unbiased sample for the efficiency measurement.
A Breit-Wigner convoluted with a Gaussian and a falling combinatoric background
is fit to the peak to estimate the contribution
of the $Z$ boson resonance in both the passing and failing probes.
A scale factor is then applied to MC to match the data efficiency.
Figure~\ref{fig:muon-tag-probe} shows one of these measurements of efficiency in Data.
\begin{figure}
  \centering
  \adjincludegraphics[width=0.3\linewidth,trim={0 0 {0.5\width} {0.5\height}},clip]{figures/fit1.png}~
  \adjincludegraphics[width=0.3\linewidth,trim={0 {0.5\height} {0.5\width} 0},clip]{figures/fit1.png}~
  \adjincludegraphics[width=0.3\linewidth,trim={{0.5\width} {0.5\height} 0 0},clip]{figures/fit1.png}
  \caption[Tag and probe fits]{
    Example tag and probe fits are shown above.
    The left plot shows the di-lepton mass fit for all tagged events.
    The tagging procedure yields little background, which allows for a precise measurement.
    The middle plot shows the events where the probe passed muon identification,
    and the right plot show the events when the probe failed.
    }
  \label{fig:muon-tag-probe}
\end{figure}
Each of the efficiency measurements and scale factors are binned
in muon $p_T$ and $\eta$.
The $p_T$ bin boundaries are $[20, 25, 30, 40, 50, 60, \infty)$ in GeV.
Bins in $|\eta|$ are delimited at $[0, 0.9, 1.2, 2.1, 2.4]$.

The uncertainties from the scale factors are also combined in the final analysis.
The identification efficiencies in data are slightly lower than in Monte Carlo.
Loosely identified muons require an average scale factor of $0.998\pm0.002$ applied to simulated events,
and tightly identified muons require an average scale factor of $0.98\pm0.005$
\cite{CMS-DP-2019-022}.
In general, the scale factors are half a percent or one percent lower, respectively,
at values of $|\eta| > 2.0$, and flat across $p_T$ bins.

\subsection{Electrons}

The electron scale factors are measured in a similar manner as the muon scale factors,
using the tag and probe method.
The relative difficulty in reconstructing electrons, which are contained in the ECAL,
compared to muons leaving tracks through the muon chambers
means that a reconstruction scale factor, $\epsilon^e_\mathrm{RECO}$, is also factored into the full scale factor.
\begin{gather}
  \epsilon^e = \epsilon^e_\mathrm{RECO} \times \epsilon^e_\mathrm{ISO + ID|RECO} \times \epsilon^e_\mathrm{Trigger|ISO + ID + RECO}
\end{gather}
The rest of the efficiency scale factors follow the same definition patterns given for the muons.
The binning for the electron scale factors is also the same as the binning for the muon scale factors.
The scale factor is around $1.0 \pm 0.02$ per electron for most of the bins.

\subsection{MET Trigger Scale Factors}

The MET trigger efficiencies for each year are measured in events with a single electron and large MET.
This allows tagging events with the single electron triggers, and selecting mostly
events from $W$+jets and $t\bar{t}$ events.
The events are additionally required to be similar to the events of interest in the analysis.
There must be two jets with $p_T > \SI{20}{GeV}$ and $|\eta| < 2.5$,
and the events must pass the MET filters.
In addition, it is required that the $\Delta\phi$ between the electron and MET is less than 2.5.
The nominal values for the scale factors are derived by measuring
the MET trigger efficiency in data and scaling the trigger efficiency in
the $t\bar{t}$ Monte Carlo sample to match.
Uncertainties are derived by repeating the measurement using a $W$+jets sample.
The scale factors for the nominal measurement and the $W$+jets sample are shown in
Figure~\ref{fig:met-sf}.
\begin{figure}
  \centering
  \raisebox{2.7cm}{\rotatebox{90}{\tiny \textsf{MET Trigger SF}}}
  \includegraphics[width=0.45\linewidth]{figures/METSF2016.pdf}~
  \raisebox{2.7cm}{\rotatebox{90}{\tiny \textsf{MET Trigger SF}}}
  \includegraphics[width=0.45\linewidth]{figures/METSF2017.pdf} \\
  \raisebox{2.5cm}{\rotatebox{90}{\tiny \textsf{MET Trigger SF}}}
  \includegraphics[width=0.45\linewidth,trim={1.5cm 0 0 0},clip]{figures/METSF2018.png}
  \caption[MET trigger scale factors]{
    The MET trigger scale factors for two varying simulations are given.
    The $t\bar{t}$ sample is used for the nominal scale factor,
    and the $W$+jets is used to estimate systematic uncertainties.
    The top left plot shows the measurements for 2016,
    the top right plot shows 2017,
    and the bottom plot shows the MET scale factors for 2018.
    Each scale factor is a function of the minimum MET or missing hadronic energy.
    Remember the MET cut for the 0-lepton channel is at \SI{170}{GeV},
    so the different behavior in 2018 below \SI{150}{GeV} does not affect the analysis.
    The kink in the 2016 $W$+jets line is caused by the merging of two fits,
    but since this is a variation to assess uncertainties, it is not necessary to fix.
  }
  \label{fig:met-sf}
\end{figure}
The MET trigger uncertainty is derived by finding the maximum deviation between the two lines above
the \SI{170}{GeV} requirement in the 0-lepton channel,
and applying that difference as a shape uncertainty.

% \phil Add b-tagging sub-section

\section{Energy Corrections of $b$-Jets}

This analysis does depend in particular on $b$-jet energy predictions,
which do not have recommended corrections from the CMS collaboration,
so a bespoke measurement is done for this analysis.
Even when distributions of individual variables agree between MC and data,
correlations are often different.
These correlations are also important in the evaluation of a DNN.
The DNN used to estimate the energy of $b$-jets therefore has differing performance
in MC and data.
In particular, it is better at estimating the true energy of a $b$-jet in MC.
The energies evaluated in MC must be smeared in order to accurately simulate
the resolution of jets in data after they have been modified by the DNN regression.

One way to measure jet energy resolution is to consider an event
where a jet is recoiling off of a $Z$ boson that decays into leptons.
In principle, the $Z$ boson's transverse momentum is balanced with the
jet's transverse momentum.
Measurements of lepton energies in the CMS detector is relatively precise,
so the ratio of the reconstructed jet's
$p_T$ to the $Z$ boson's $p_T$ allows measurement of the jet energy resolution.
Ideally, this measurement would be done with an collision resulting in one $Z$ boson decay,
and one jet.
However, this is an infrequent occurrence.
Instead, events with two jets are selected, with one jet having relatively low $p_T$.
A fit is performed to estimate resolution characteristics
where the second jet would have $p_T = \SI{0}{GeV}$.

These events are selected using the following requirements:

\begin{itemize}
\item Exactly two muons or two electrons must pass the selection criteria for the
  di-leptons channels described in Section~\ref{sec:resolved-2}.
\item The two selected leptons must be oppositely charged.
\item The di-lepton kinematics must satisfy \\ $p_{T,\ell\ell} > \SI{100}{GeV}$ and
  $\SI{71}{GeV} < m_{\ell\ell} < \SI{111}{GeV}$.
\item Exactly two jets must pass the pre-selection described in Section~\ref{sec:resolved-2}.
\item The leading jet must also satisfy $\Delta\phi(j, \ell\ell) > 2.8$
\item The ratio between the sub-leading jet $p_T$ and
  the di-lepton $p_T$ must be less than 0.3.
\item The leading jet must pass the tight working point for the $b$-tagger,
  as defined for each year in Table~\ref{tab:deepcsv}.
\end{itemize}

The selected events are divided into four bins of $\alpha = p_{T,j2}/p_{T, \ell\ell}$
with bounds $(0, 0.155, 0.185, 0.23, 0.3)$.
The distribution of $\alpha$ is shown in Figure~\ref{fig:alpha}, along with the bin boundaries.
These bins were selected to give approximately the same number of data events in each bin.
%
\begin{figure}
  \centering
  \includegraphics[width=0.6\linewidth]{figures/200303_alpha_lines_v2/smearplot_alpha.pdf}
  \caption[Distribution of $\alpha$ used for $b$ jet energy smearing]{
    The distribution for $\alpha$ is shown above, along with the binning boundaries.
  }
\end{figure}
%
The jet response ($p_{T, j1}/p_{T, \ell\ell + j2}$) is plotted in each bin,
with uncertainties from renormalization and refactorization scale weights
and parton shower weights.
These histograms of jet response are shown in Fig.~\ref{fig:jetsmear-responses}.
From each plot, the mean ($\mu$) and the standard deviation ($\sigma$) are extracted.
The relative resolution ($\sigma/\mu$) is fit as a function of $\alpha$.

\begin{figure}
  \centering
  \includegraphics[width=0.23\linewidth]{figures/201019_2016/smearplot_1_jet1_adjusted_response.pdf}
  \includegraphics[width=0.23\linewidth]{figures/201019_2016/smearplot_2_jet1_adjusted_response.pdf}
  \includegraphics[width=0.23\linewidth]{figures/201019_2016/smearplot_3_jet1_adjusted_response.pdf}
  \includegraphics[width=0.23\linewidth]{figures/201019_2016/smearplot_4_jet1_adjusted_response.pdf} \\
  \includegraphics[width=0.23\linewidth]{figures/201019_2017/smearplot_1_jet1_adjusted_response.pdf}
  \includegraphics[width=0.23\linewidth]{figures/201019_2017/smearplot_2_jet1_adjusted_response.pdf}
  \includegraphics[width=0.23\linewidth]{figures/201019_2017/smearplot_3_jet1_adjusted_response.pdf}
  \includegraphics[width=0.23\linewidth]{figures/201019_2017/smearplot_4_jet1_adjusted_response.pdf} \\
  \includegraphics[width=0.23\linewidth]{figures/201019_2018/smearplot_1_jet1_adjusted_response.pdf}
  \includegraphics[width=0.23\linewidth]{figures/201019_2018/smearplot_2_jet1_adjusted_response.pdf}
  \includegraphics[width=0.23\linewidth]{figures/201019_2018/smearplot_3_jet1_adjusted_response.pdf}
  \includegraphics[width=0.23\linewidth]{figures/201019_2018/smearplot_4_jet1_adjusted_response.pdf} \\
  \caption[Response to evaluate jet smearing]{
    The histograms of response for each event are shown above.
    The top row shows 2016, the middle shows 2017, and the bottom row shows 2018 histograms.
  }
  \label{fig:jetsmear-responses}
\end{figure}

\begin{gather}
  f(\alpha) = (m \times \alpha) \oplus b \times (1 + c_k \times \alpha)
\end{gather}

The $c_k$ term is fixed by a linear fit to the MC's intrinsic jet resolution
($p_{T, reco}/p_{T, gen}$) over $\alpha$ as $c_k = m_0/q_0$.
The fit results are shown in Fig.~\ref{fig:jetsmear-fits}.
The resolution is corrected by scaling the difference between
$p_{T,reco}$ and $p_{T,gen}$ by $b_{data}/b_{MC}$.
This causes the post-smearing fits to agree at $\alpha = 0$.
Uncertainties are extracted from the fit uncertainties of $b$ for data and MC.
The resulting smearing values are in Table~\ref{tab:jetsmear-res}.
The difference in smearing values for 2016 can be explained by a few characteristics of the simulation and data.
First, the 2016 data has been more thoroughly studied and can generally be expected to have more robust calibration.
The detector behavior also degrades over time as it accumulates radiation damage. 
Though this is accounted for in calibrating,
it does make more precise simulations of detector response less accurate.
Finally, the simulation for 2016 uses a different \texttt{PYTHIA} tune.

\begin{figure}
  \centering
  \includegraphics[width=0.3\linewidth]{figures/201015_smear_201015_2016_tight_divmean/resolution_jet1_adjusted_response_smear_0.pdf}
  \includegraphics[width=0.3\linewidth]{figures/201012_smear_201012_2017_tight_divmean/resolution_jet1_adjusted_response_smear_0.pdf}
  \includegraphics[width=0.3\linewidth]{figures/201004_smear_201002_2018_divmean/resolution_jet1_adjusted_response_smear_0.pdf}
  \caption[Resolution fits for jet smearing]{
    The fits to Data, MC, and intrinsic resolutions are shown.
    From left to right are the fits for 2016, 2017, and 2018.
  }
  \label{fig:jetsmear-fits}
\end{figure}

\begin{table}
  \centering
  \caption[$b$-jet energy correction parameters]{
    The extracted corrections needed for each year of data as a percent of the jet's $p_T$.
  }
  \begin{tabular}{c|c|c}
    \hline
    Year & Scaling & Resolution Difference \\
    \hline
    2016 & $0.998 \pm 0.019$ & $0.017 \pm 0.060$ \\
    2017 & $1.020 \pm 0.023$ & $0.088 \pm 0.071$ \\
    2018 & $0.985 \pm 0.019$ & $0.080 \pm 0.073$ \\
    \hline
  \end{tabular}
  \label{tab:jetsmear-res}
\end{table}

\section{Theoretical Corrections}

There are known inaccuracies in the simulations used in this analysis
that can be understood at the theoretical level.
However, less accurate calculations are run because they are simpler
and faster to compute the results for.
A trade-off must be made between the accuracy of the simulation
and being able to generate enough simulated events to fill the phase space
relevant for all analyses CMS collaborators are undertaking.

\subsection{LO to NLO Reweighting}

For each $V$+jets process, $Z\rightarrow\nu\nu$, $W\rightarrow\ell\nu$,
and $Z\rightarrow\ell\ell$, there are three distinct processes that are simulated separately.
\begin{itemize}
\item $b$-quarks are generated in the matrix element
\item $b$-quarks are not in the matrix element, but are produced in the parton shower
\item No $b$-quarks are present in the event
\end{itemize}
For the first two, VBJets and VJetsBGenFilter datasets are used respectively,
as listed in Appendix~\ref{app:generator},
but they are only generated for $p_T(V) > \SI{100}{GeV}$.
This small phase space and low relative cross section for generating $b$ quarks
means that it is feasible to simulate these processes to Next to Leading Order (NLO) diagrams.
However, expected events with no $b$-quarks and events with $p_T(V) < \SI{100}{GeV}$
are more numerous, so more simulated events must be generated.
To speed up calculations, Leading Order (LO) samples are used for these processes.

One common way to make LO samples more accurate
is to generate NLO in QCD samples and reweight the LO sample
to match generator-level kinematics.
This can be done with an inclusive selection so that the statistical limitations
of the available NLO samples are not significant.
While the samples in this analysis were generated using just
MadGraph5 \cite{hirschi2015automated},
the following inclusive samples using aMC@NLO \cite{Alwall:2014hca}
are used to reweight the LO simulation as a function of $p_T(V)$.
\begin{itemize}
\item Z1JetsToNuNu\_M-50\_LHEZpT\_50-150\_TuneCP5\_13TeV-amcnloFXFX-pythia8
\item Z1JetsToNuNu\_M-50\_LHEZpT\_150-250\_TuneCP5\_13TeV-amcnloFXFX-pythia8
\item Z1JetsToNuNu\_M-50\_LHEZpT\_250-400\_TuneCP5\_13TeV-amcnloFXFX-pythia8
\item Z2JetsToNuNu\_M-50\_LHEZpT\_50-150\_TuneCP5\_13TeV-amcnloFXFX-pythia8
\item Z2JetsToNuNu\_M-50\_LHEZpT\_150-250\_TuneCP5\_13TeV-amcnloFXFX-pythia8
\item Z2JetsToNuNu\_M-50\_LHEZpT\_250-400\_TuneCP5\_13TeV-amcnloFXFX-pythia8
\item WJetsToLNu\_0J\_TuneCP5\_13TeV-amcatnloFXFX-pythia8
\item WJetsToLNu\_1J\_TuneCP5\_13TeV-amcatnloFXFX-pythia8
\item WJetsToLNu\_2J\_TuneCP5\_13TeV-amcatnloFXFX-pythia8
\item DYJetsToLL\_M-50\_TuneCP5\_13TeV-amcatnloFXFX-pythia8
\end{itemize}
The change to the $p_T(V)$ spectra and the resulting improved agreement between data and MC
are shown in Figure~\ref{fig:nlo-reweight}.
\begin{figure}
  \centering
  \includegraphics[width=0.45\linewidth]{figures/Vjets_NLOreweighting_2017V5_Znn_withoutWeight.pdf}~
  \includegraphics[width=0.45\linewidth]{figures/Vjets_NLOreweighting_2017V5_Znn_withWeight.pdf}
  \includegraphics[width=0.45\linewidth]{figures/Vjets_NLOreweighting_2017V5_Wln_withoutWeight.pdf}~
  \includegraphics[width=0.45\linewidth]{figures/Vjets_NLOreweighting_2017V5_Wln_withWeight.pdf}
  \includegraphics[width=0.45\linewidth]{figures/Vjets_NLOreweighting_2017V5_Zll_withoutWeight.pdf}~
  \includegraphics[width=0.45\linewidth]{figures/Vjets_NLOreweighting_2017V5_Zll_withWeight.pdf}
  \caption[LO to NLO reweighting shape comparisons]{
    The shapes for $p_T(V)$ in the V+light control regions are shown for 2017.
    The left plots are before LO to NLO reweighting,
    and the right plots are after the correction factor is applied.
    The top row of plots show the 0-lepton control region,
    the middle row shows the 1-lepton selection,
    and the bottom row shows the 2-lepton control region.
  }
  \label{fig:nlo-reweight}
\end{figure}

Note that these NLO corrections are only applied to the HT-binned $V$ + jets samples.
These samples are also combined with $b$-enriched samples that are already simulated in NLO.
It is common to remove events with simulated $b$ hadrons from the HT-binned samples in this case
and use only the $b$-enriched samples for those events.
However this is not possible in this analysis because there are not enough simulated events
with high vector boson $p_T$ in the $b$-enriched samples.
Therefore, both the NLO samples that are used to reweight the LO samples also need to
simulate events containing $b$ hadrons.

\subsection{Electroweak and QCD Corrections to Background}

Though the LO to NLO reweighting does improve the agreement between MC and data,
there is also an additional small correction.
Higher order electroweak and QCD corrections predict a slightly softer $p_T(V)$ spectrum
\cite{Kallweit_2016}.
The reweighting function is shown in Figure~\ref{fig:EWKcorr}.

\begin{figure}
  \centering
  \includegraphics[width=0.5\textwidth]{figures/EWKcorr.pdf}
  \caption[Electroweak corrections to background]{
    Electroweak correction as a function of boson $p_T(V)$ for the V+jets samples.
  }
  \label{fig:EWKcorr}
\end{figure}


\subsection{Corrections to Signal Process}

The samples produced for signal are NLO.
For an accurate measurement of the $V\!H$ cross section,
the simulation is scaled to Next to Next to Leading Order (NNLO) QCD effects,
where two gluons are added to the Feynman diagrams.
An example NNLO VH process is shown in Figure~\ref{fig:nnlo-qcd}.
\begin{figure}
  \centering
  \begin{fmffile}{nnlo_qcd}
    \fmfframe(0,0)(0, 20){
    \begin{fmfgraph*}(250, 150)
      \fmfleft{i0,i1}
      \fmfright{o0,o1,o2,o3,og0}
      \fmf{quark}{i1,vg0,vg1,v0,vg2,i0}
      \fmf{gluon,tension=0}{vg0,vg2}
      \fmf{gluon,tension=0}{vg1,og0}
      \fmf{boson}{v0,v1,v2}
      \fmf{fermion}{o0,v2,o1}
      \fmf{dashes}{v1,v3}
      \fmf{fermion}{o2,v3,o3}
    \end{fmfgraph*}
    }
  \end{fmffile}
  \caption[$V\!H$ in NNLO QCD]{
    Above is an example diagram used to calculate NNLO QCD corrections to the
    $V\!H$ production cross section.
  }
  \label{fig:nnlo-qcd}
\end{figure}
Electroweak corrections can still be factored out,
and total $V\!H$ production cross sections are given as the following
\cite{DeFlorianSabaris:2215893}.
\begin{gather}
  \sigma^{W\!H} = \sigma^{W\!H,DY}_\mathrm{NNLOQCD} (1 + \delta_\mathrm{EWK}) + \sigma_{t-\mathrm{loop}} + \sigma_\gamma \\
  \sigma^{Z\!H} = \sigma^{Z\!H,DY}_\mathrm{NNLOQCD} (1 + \delta_\mathrm{EWK}) + \sigma_{t-\mathrm{loop}} + \sigma_\gamma + \sigma^{ggZ\!H}
\end{gather}
Each $\sigma$ on the RHS of the equations refers to a different Higgs production mechanism.
The full correction shape as a function of $p_T(V)$ is shown in
Figure~\ref{fig:signal-corr-shape}.
\begin{figure}
  \centering
  \includegraphics[width=0.45\linewidth]{figures/Elektroweak_signal_correction_Wp.pdf} ~
  \includegraphics[width=0.45\linewidth]{figures/Elektroweak_signal_correction_Zll.pdf}
  \caption[Full corrections to signal samples]{
    The full set of corrections to two signal samples are shown above.
    On the left is production of the Higgs with a $W^+$ boson.
    On the right is Higgs production with a $Z$ decaying to two leptons.
  }
  \label{fig:signal-corr-shape}
\end{figure}

\section{Application of Uncertainties}

In addition to the corrections and their associated uncertainties listed so far,
there are additional experimental and theoretical uncertainties.
Some of these uncertainties only affect the normalization of samples.
Other uncertainties affect the shape of variables that are used in the DNN
discriminator described in the following chapter.
The final set of uncertainties is a set of migration uncertainties to account for
variation near the STXS bin boundaries.

\subsection{Normalization Uncertainties}

The following uncertainties only affect the normalization of the simulated samples.
\begin{itemize}
\item The luminosity measurement has an uncertainty of
  2.5\% for 2016 and 2018, and 2.3\% for 2017
  \cite{CMS-PAS-LUM-17-001,CMS-PAS-LUM-17-004,CMS-PAS-LUM-18-002}.
  Since the method of luminosity measurement was the same for all three years,
  the uncertainties are partially correlated across the three years.
\item The theoretical uncertainty of the branching ratio of the Higgs Boson
  to bottom quarks is 0.5\% \cite{DeFlorianSabaris:2215893}.
\item QCD scale uncertainties for the signal production cross section
  are implemented as acceptance uncertainties.
\item The uncertainties to the proton's PDF and $\alpha_s$ for the signal processes
  are 1.6\% for $Z\!H$ production, and 1.9\% for $W\!H$ production.
\item At the high $p_T(V)$ regions of this analysis, the theoretical uncertainties grow.
  For the NLO electroweak corrections, the uncertainties are 2\%.
  For the NNLO QCD correction, the uncertainty is 5\%.
\item For smaller background processes without a dedicated control region,
  primarily single-top and di-boson processes, a 15\% normalization uncertainty is applied.
  This number comes from measured cross sections for single-top \cite{2017752}
  and di-boson \cite{2017533}.
\item The lepton identification efficiency uncertainties are applied as a flat uncertainty
  to each channel, as appropriate.
\item The MET trigger efficiencies are also applied as a normalization uncertainty
  to the 0-lepton regions, as mentioned before.
\end{itemize}

\subsection{Shape Uncertainties}

In addition to normalization, there is some uncertainty in the shapes of the various distributions that are fit.
This is typically caused by uncertainties in variables that are used in the event selection,
but can also be a result of normalization uncertainties that only affect a subset of events in a selection.
The shape variations are generated by varying different parameters up and down
and re-applying selections and weights to determine the final shape of each distribution.
Uncertainties of simulated processes that cause shape differences are the following.
\begin{itemize}
\item The energy resolution, and therefore the exact final energy,
  of simulated $b$ jets are varied by increasing or decreasing
  the distance between the generator-level $p_T$ and the reconstructed jet $p_T$.
\item The energy scale is varied by varying all jet energies up and down by
  one standard deviation \cite{Khachatryan_2017}.
\item The $b$-tagging efficiency uncertainties are calculated
  for multiple $p_T$ and $\eta$ bins.
  The uncertainties for these different bins are uncorrelated \cite{btagsys},
  so they each affect a different subset of events when applied.
\item The $b$-tagging efficiencies for fat jets are applied in bins of $p_T$ \cite{CMS-DP-2018-046}.
  The uncertainties are decorrelated in these $p_T$ bins.
\item The uncertainties from the limited number of Monte Carlo events is handled
  using an approximation of the Barlow-Beeston method \cite{BARLOW1993219}.
\end{itemize}

\subsection{Migration Uncertainties}

An additional uncertainty is a applied that accounts for simulated events crossing the STXS bin boundaries
due to uncertainties in the vector boson $p_T$.
The effect of each nuisance parameter is anti-correlated between the bins on either side of the boundary.
Most bins have two uncertainties allowing events to cross into lower or higher $p_T(V)$ bins,
with the exception of the highest STXS bins, which have no upper bounds.
Due to limited availability of studies on this effect,
these systematic uncertainties are given large values before the global fit is performed,
ranging from 20\% to 50\% for each individual boundary.
These are worth noting since they have the largest affect of all systematic uncertainties
on the pre-fit simulation uncertainties,
though they are significantly constrained by the fit process.
